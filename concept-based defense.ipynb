{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bkE8IeGkX769"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.autofs/tools/spack/var/spack/environments/default-ml-x86_64-24071101/.spack-env/view/lib/python3.11/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "2025-08-17 05:35:54.068547: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-17 05:35:58.450688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/.autofs/tools/spack/var/spack/environments/default-ml-x86_64-24071101/.spack-env/view/lib/python3.11/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name resnetv2_50x1_bit_distilled to current resnetv2_50x1_bit.goog_distilled_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import colorsys\n",
    "import warnings\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = timm.create_model(\"resnetv2_50x1_bit_distilled\", pretrained=False, num_classes=10)\n",
    "    \n",
    "# Load weights\n",
    "model.reset_classifier(num_classes=10)\n",
    "checkpoint = torch.load(\"./benchmark/models/resnetv2_50x1_bit_distilled_imagenette.pth\")\n",
    "model.load_state_dict(checkpoint['state_dict']) \n",
    "\n",
    "# Move to device and set eval mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# loading dataloaders\n",
    "dataloaders = get_imagenette_dataloaders(\"imagenette2/\", batch_size=64)\n",
    "num_classes = 10\n",
    "\n",
    "sorted_dataset = [[] for _ in range(num_classes)]\n",
    "\n",
    "for batch_id, (images, labels) in enumerate(dataloaders['test']):\n",
    "    images = images.to(device)\n",
    "    output = model(images)\n",
    "    _, preds = torch.max(output, dim=1)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        sorted_dataset[preds[i]].append(images[i, :])\n",
    "        \n",
    "        \n",
    "# Class labels:\n",
    "class_labels = {\n",
    "    0: \"tench\", \n",
    "    1: \"English springer\", \n",
    "    2: \"cassette player\", \n",
    "    3: \"chain saw\", \n",
    "    4: \"church\", \n",
    "    5: \"French horn\", \n",
    "    6: \"garbage truck\", \n",
    "    7: \"gas pump\", \n",
    "    8: \"golf ball\", \n",
    "    9: \"parachute\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_attacked_dataset_1 = torch.load(\"sorted_attacked_dataset_1\")\n",
    "sorted_attacked_preds_1 = torch.load(\"sorted_attacked_preds_1\")\n",
    "\n",
    "sorted_attacked_dataset_2 = torch.load(\"sorted_attacked_dataset_2\")\n",
    "sorted_attacked_preds_2 = torch.load(\"sorted_attacked_preds_2\")\n",
    "\n",
    "sorted_attacked_dataset_3 = torch.load(\"sorted_attacked_dataset_3\")\n",
    "sorted_attacked_preds_3 = torch.load(\"sorted_attacked_preds_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250\n"
     ]
    }
   ],
   "source": [
    "size_1 = 0\n",
    "for i in range(10):\n",
    "    size_1 += len(sorted_attacked_dataset_1[i])\n",
    "print(size_1)\n",
    "\n",
    "size_2 = 0\n",
    "for i in range(10):\n",
    "    size_2 += len(sorted_attacked_dataset_2[i])\n",
    "print(size_2)\n",
    "\n",
    "size_3 = 0\n",
    "for i in range(10):\n",
    "    size_3 += len(sorted_attacked_dataset_3[i])\n",
    "print(size_3)\n",
    "\n",
    "size = 0\n",
    "for i in range(10):\n",
    "    size += len(sorted_dataset[i])\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bd4f8734bc48af97fceabcea2c432f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accuracy:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=size, desc=\"Accuracy\") as pbar:\n",
    "    for batch_id, (images, labels) in enumerate(dataloaders['test']):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        original_preds = model(images).argmax(dim=1)\n",
    "        \n",
    "        pbar.update((original_preds == labels).sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defense class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Craft.craft.craft_torch import Craft, torch_to_numpy\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "def blurring_multi_masking(images, images_u, most_important_concepts, percentile, num_concepts=2):\n",
    "    masked_images = []\n",
    "\n",
    "    for id in range(images.shape[0]):\n",
    "        img = images[id].clone()\n",
    "        final_mask = torch.zeros_like(img[0], dtype=torch.bool)\n",
    "        for c_id in most_important_concepts[:num_concepts]:\n",
    "            heatmap = torch.tensor(images_u[id, :, :, c_id])\n",
    "            sigma = torch.quantile(heatmap.flatten(), percentile / 100.0)\n",
    "            concept_mask = cv2.resize((heatmap > sigma).cpu().numpy().astype(np.uint8), \n",
    "                                      (img.shape[2], img.shape[1])).astype(bool)\n",
    "            \n",
    "            final_mask = final_mask | torch.from_numpy(concept_mask)\n",
    "\n",
    "        if final_mask.any(): \n",
    "            final_mask_3d = final_mask.unsqueeze(0).repeat(img.shape[0], 1, 1) \n",
    "            \n",
    "            blurred_img = transforms.functional.gaussian_blur(img, kernel_size=(21, 21), sigma=(5.0, 5.0))\n",
    "            img[final_mask_3d] = blurred_img[final_mask_3d]\n",
    "\n",
    "        masked_images.append(img)\n",
    "\n",
    "    masked_images = torch.stack(masked_images).to(device)\n",
    "    return masked_images\n",
    "\n",
    "\n",
    "    \n",
    "class ConceptXAIBasedDefense:        \n",
    "    def __init__(self, model, device, batch_size):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        def g(x):\n",
    "            feats = self.model.forward_features(x)  \n",
    "            return F.relu(feats) \n",
    "        def h(z):\n",
    "            return self.model.forward_head(z, pre_logits=False)\n",
    "        \n",
    "\n",
    "        self.craft = Craft(input_to_latent = g,\n",
    "              latent_to_logit = h,\n",
    "              number_of_concepts = 10,\n",
    "              patch_size = 64,\n",
    "              batch_size = batch_size,\n",
    "              device = device)\n",
    "        \n",
    "    def __call__(self, images, defense_function, class_id, percentile, num_concepts=2):\n",
    "        crops, crops_u, w = self.craft.fit(images)\n",
    "        crops = np.moveaxis(torch_to_numpy(crops), 1, -1)\n",
    "        \n",
    "        importances = self.craft.estimate_importance(images, class_id=class_id)\n",
    "        images_u = self.craft.transform(images)\n",
    "        images = images.detach().cpu()\n",
    "        \n",
    "        most_important_concepts = np.argsort(importances)[::-1][:5]\n",
    "        \n",
    "        return defense_function(images, images_u, most_important_concepts, percentile, num_concepts)\n",
    "    \n",
    "    \n",
    "def robust_accuracy(sorted_dataset, sorted_preds, size, function):\n",
    "    defense = ConceptXAIBasedDefense(model, device, batch_size=16) \n",
    "\n",
    "    for concepts in range(1, 6): \n",
    "        print(f\"Testing with concept {concepts}\")        \n",
    "        with tqdm(total=size, desc=\"Recover Rate\") as pbar:\n",
    "            for j in range(num_classes): \n",
    "                images_class = sorted_dataset[j]\n",
    "                labels_class = torch.stack(sorted_preds[j])\n",
    "\n",
    "                images = torch.stack(images_class)\n",
    "                \n",
    "                class_recovered = 0\n",
    "                \n",
    "                for index in range(0, len(images), 16):\n",
    "                    batch_images = images[index:index+16].to(device)\n",
    "                    batch_labels = labels_class[index:index+16].to(device)\n",
    "                    \n",
    "                    masked_images = defense(batch_images, function, j, 90, concepts).to(device)\n",
    "                    output = model(masked_images)\n",
    "                    \n",
    "                    _, preds = torch.max(output, dim=1)\n",
    "\n",
    "                    recovered = (preds == batch_labels).sum().item()\n",
    "                    class_recovered += recovered\n",
    "                    pbar.update(recovered)\n",
    "                \n",
    "                print(f\"Class {j+1} | Recovered: {class_recovered}/{len(images)} | Accuracy: {class_recovered/len(images):.4f}\")\n",
    "                \n",
    "                \n",
    "def clean_accuracy(function):\n",
    "    for percentile in range(90, 100): \n",
    "        print(f\"Testing with percentile {percentile}\")    \n",
    "        with tqdm(total=3925, desc=\"Recover Rate\") as pbar:\n",
    "            for j in range(num_classes):\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                images = sorted_dataset[j]\n",
    "                images = torch.stack(images)\n",
    "                images = images.to(device)\n",
    "\n",
    "                batch_size = 64\n",
    "                class_recovered = 0\n",
    "\n",
    "                for index in range(0, len(images), batch_size):\n",
    "                    output = model(images[index:index+batch_size])\n",
    "                    _, original_preds = torch.max(output, dim=1)\n",
    "\n",
    "                    defense = ConceptXAIBasedDefense(model, device, len(images[index:index+batch_size]))\n",
    "                    masked_images = defense(images[index:index+batch_size], function, j, percentile, 2).to(device)\n",
    "\n",
    "                    output = model(masked_images)\n",
    "                    _, preds = torch.max(output, dim=1)\n",
    "\n",
    "                    recovered = (preds == original_preds).sum().item()\n",
    "                    class_recovered += recovered\n",
    "                    pbar.update(recovered)\n",
    "                print(f\"Class Accuracy {j}: {class_recovered/len(images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  blurring multimasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2% patch\n",
      "Testing with concept 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced4a14256b94c6c83b8c9685c55ab94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 | Recovered: 47/47 | Accuracy: 1.0000\n",
      "Class 2 | Recovered: 29/30 | Accuracy: 0.9667\n",
      "Class 3 | Recovered: 208/222 | Accuracy: 0.9369\n",
      "Class 4 | Recovered: 198/201 | Accuracy: 0.9851\n",
      "Class 5 | Recovered: 61/64 | Accuracy: 0.9531\n",
      "Class 6 | Recovered: 83/85 | Accuracy: 0.9765\n",
      "Class 7 | Recovered: 96/99 | Accuracy: 0.9697\n",
      "Class 8 | Recovered: 184/196 | Accuracy: 0.9388\n",
      "Class 9 | Recovered: 151/152 | Accuracy: 0.9934\n",
      "Class 10 | Recovered: 2104/2154 | Accuracy: 0.9768\n",
      "Testing with concept 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91890ad5fdd849909b11150a15dfc2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 | Recovered: 46/47 | Accuracy: 0.9787\n",
      "Class 2 | Recovered: 30/30 | Accuracy: 1.0000\n",
      "Class 3 | Recovered: 213/222 | Accuracy: 0.9595\n",
      "Class 4 | Recovered: 194/201 | Accuracy: 0.9652\n",
      "Class 5 | Recovered: 59/64 | Accuracy: 0.9219\n",
      "Class 6 | Recovered: 83/85 | Accuracy: 0.9765\n",
      "Class 7 | Recovered: 97/99 | Accuracy: 0.9798\n",
      "Class 8 | Recovered: 187/196 | Accuracy: 0.9541\n",
      "Class 9 | Recovered: 151/152 | Accuracy: 0.9934\n",
      "Class 10 | Recovered: 2094/2154 | Accuracy: 0.9721\n",
      "Testing with concept 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b93ee82f746427698c8d64ddb32c548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 | Recovered: 45/47 | Accuracy: 0.9574\n",
      "Class 2 | Recovered: 27/30 | Accuracy: 0.9000\n",
      "Class 3 | Recovered: 212/222 | Accuracy: 0.9550\n",
      "Class 4 | Recovered: 193/201 | Accuracy: 0.9602\n",
      "Class 5 | Recovered: 57/64 | Accuracy: 0.8906\n",
      "Class 6 | Recovered: 82/85 | Accuracy: 0.9647\n",
      "Class 7 | Recovered: 94/99 | Accuracy: 0.9495\n",
      "Class 8 | Recovered: 184/196 | Accuracy: 0.9388\n",
      "Class 9 | Recovered: 150/152 | Accuracy: 0.9868\n",
      "Class 10 | Recovered: 2110/2154 | Accuracy: 0.9796\n",
      "Testing with concept 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f7c9d9008a4d739443a2145dca6ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 | Recovered: 44/47 | Accuracy: 0.9362\n",
      "Class 2 | Recovered: 27/30 | Accuracy: 0.9000\n",
      "Class 3 | Recovered: 211/222 | Accuracy: 0.9505\n",
      "Class 4 | Recovered: 189/201 | Accuracy: 0.9403\n",
      "Class 5 | Recovered: 57/64 | Accuracy: 0.8906\n",
      "Class 6 | Recovered: 82/85 | Accuracy: 0.9647\n",
      "Class 7 | Recovered: 91/99 | Accuracy: 0.9192\n",
      "Class 8 | Recovered: 185/196 | Accuracy: 0.9439\n",
      "Class 9 | Recovered: 145/152 | Accuracy: 0.9539\n",
      "Class 10 | Recovered: 2086/2154 | Accuracy: 0.9684\n",
      "Testing with concept 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8789820e88422db5ebf9eb89c196b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 | Recovered: 44/47 | Accuracy: 0.9362\n",
      "Class 2 | Recovered: 27/30 | Accuracy: 0.9000\n",
      "Class 3 | Recovered: 213/222 | Accuracy: 0.9595\n",
      "Class 4 | Recovered: 182/201 | Accuracy: 0.9055\n",
      "Class 5 | Recovered: 56/64 | Accuracy: 0.8750\n",
      "Class 6 | Recovered: 81/85 | Accuracy: 0.9529\n",
      "Class 7 | Recovered: 84/99 | Accuracy: 0.8485\n",
      "Class 8 | Recovered: 184/196 | Accuracy: 0.9388\n",
      "Class 9 | Recovered: 145/152 | Accuracy: 0.9539\n",
      "Class 10 | Recovered: 2063/2154 | Accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"1% patch\")\n",
    "robust_accuracy(sorted_attacked_dataset_1, sorted_attacked_preds_1, size_1, blurring_multi_masking)\n",
    "print(\"2% patch\")\n",
    "robust_accuracy(sorted_attacked_dataset_2, sorted_attacked_preds_2, size_2, blurring_multi_masking)\n",
    "print(\"3% patch\")\n",
    "robust_accuracy(sorted_attacked_dataset_3, sorted_attacked_preds_3, size_3, blurring_multi_masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with concepts 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00538b28a175478bba2d15f456803746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy 0: 0.9432432432432433\n",
      "Class Accuracy 1: 0.9040767386091128\n",
      "Class Accuracy 2: 0.944954128440367\n",
      "Class Accuracy 3: 0.7993197278911565\n",
      "Class Accuracy 4: 0.9622166246851386\n",
      "Class Accuracy 5: 0.8851540616246498\n",
      "Class Accuracy 6: 0.9084158415841584\n",
      "Class Accuracy 7: 0.9084380610412927\n",
      "Class Accuracy 8: 0.9744897959183674\n",
      "Class Accuracy 9: 0.9390243902439024\n",
      "Testing with concepts 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e3e213d2524e198e27fa26e5cf9d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy 0: 0.927027027027027\n",
      "Class Accuracy 1: 0.8848920863309353\n",
      "Class Accuracy 2: 0.9357798165137615\n",
      "Class Accuracy 3: 0.7721088435374149\n",
      "Class Accuracy 4: 0.9370277078085643\n",
      "Class Accuracy 5: 0.8487394957983193\n",
      "Class Accuracy 6: 0.8811881188118812\n",
      "Class Accuracy 7: 0.8402154398563735\n",
      "Class Accuracy 8: 0.951530612244898\n",
      "Class Accuracy 9: 0.9243902439024391\n",
      "Testing with concepts 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe4c9118d2c439cb02128b3fc87454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy 0: 0.9243243243243243\n",
      "Class Accuracy 1: 0.8729016786570744\n",
      "Class Accuracy 2: 0.908256880733945\n",
      "Class Accuracy 3: 0.7448979591836735\n",
      "Class Accuracy 4: 0.9269521410579346\n",
      "Class Accuracy 5: 0.8011204481792717\n",
      "Class Accuracy 6: 0.8985148514851485\n",
      "Class Accuracy 7: 0.8150807899461401\n",
      "Class Accuracy 8: 0.9464285714285714\n",
      "Class Accuracy 9: 0.9219512195121952\n",
      "Testing with concepts 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108780bb04d0430390f504b25d78794c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy 0: 0.9216216216216216\n",
      "Class Accuracy 1: 0.8465227817745803\n",
      "Class Accuracy 2: 0.9143730886850153\n",
      "Class Accuracy 3: 0.717687074829932\n",
      "Class Accuracy 4: 0.8992443324937027\n",
      "Class Accuracy 5: 0.7843137254901961\n",
      "Class Accuracy 6: 0.9034653465346535\n",
      "Class Accuracy 7: 0.7863554757630161\n",
      "Class Accuracy 8: 0.9413265306122449\n",
      "Class Accuracy 9: 0.9048780487804878\n",
      "Testing with concepts 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8888cc330f0b4c358707cf8545f8da89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recover Rate:   0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy 0: 0.9135135135135135\n",
      "Class Accuracy 1: 0.8321342925659473\n",
      "Class Accuracy 2: 0.9021406727828746\n",
      "Class Accuracy 3: 0.7244897959183674\n",
      "Class Accuracy 4: 0.8841309823677582\n",
      "Class Accuracy 5: 0.7703081232492998\n",
      "Class Accuracy 6: 0.8960396039603961\n",
      "Class Accuracy 7: 0.7612208258527827\n",
      "Class Accuracy 8: 0.9464285714285714\n",
      "Class Accuracy 9: 0.9219512195121952\n"
     ]
    }
   ],
   "source": [
    "clean_accuracy(blurring_multi_masking)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
